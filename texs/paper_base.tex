\documentclass[senior,final,11pt]{iscs-thesis}
%論文の種類とフォントサイズをオプションに
%\usepackage{graphicx}% 必要に応じて
%\usepackage{mysettings}% 自分用設定
%-------------------
\etitle{Neural Network based Decompiler for Machine Coding}
\jtitle{ニューラルネットを用いた機械語のための逆コンパイラ}

\eauthor{Sota Sato}
\jauthor{佐藤聡太}
\esupervisor{Masashi Sugiyama}
\jsupervisor{杉山将}
\supervisortitle{Professor} % Professor, etc.

\date{December 11, 2018}
%-------------------


\newcommand{\argmax}{\mathop{\rm arg\,max}\limits}
\newcommand{\argmin}{\mathop{\rm arg\,smin}\limits}

\usepackage{listings}
\usepackage{cite}
\usepackage{url}

\lstdefinestyle{myCustomMatlabStyle}{
  language=C,
  numbers=none,
  stepnumber=1,
  numbersep=10pt,
  tabsize=2,
  showspaces=false,
  showstringspaces=false
}


\lstset{%
  basicstyle={\tiny},%
  commentstyle={\tiny\itshape},%
  keywordstyle={\tiny\bfseries},%
  % style={myCustomMatlabStyle},
  stringstyle={\tiny\ttfamily},
  frame={tb},
  breaklines=true,
  columns=[l]{fullflexible},%
  numbers=left,%
  xrightmargin=0zw,%
  xleftmargin=3zw,%
  numberstyle={\scriptsize},%
  stepnumber=1,
  numbersep=1zw,%
  lineskip=-0.5ex%
}



\begin{document}
\begin{eabstract}
A decompiler is a tool for recovering a source code from compiled binary data.
There are various decompilers, but they often output a source code that is not intelligible to humans. 
In this thesis, we tried to apply machine translation techniques to generate human intelligible decompiled source codes. 
More specifically, we propose to use recurrent neural networks with attention, which are useful for machine translation. 
In experiments, we use source codes collected from open source projects and their binary data for training and evaluating the neural networks.
\end{eabstract}
\begin{jabstract}
逆コンパイラはコンパイル後のバイナリデータからソースコードを復元するためのツールである。
様々な逆コンパイラが存在するが、既存の逆コンパイラはしばしば人間にとって分かりにくいソースコードを出力する。
そこで本論文では、統計的機械翻訳の技術を逆コンパイラに用いた、解析者にとってわかりやすいソースコードを出力する逆コンパイラを提案する。
具体的には、機械翻訳において有用とされる注意機構付き再帰型ニューラルネットワークを用いる。
実験では、オープンソースプロジェクトから収集したソースコードとそのバイナリデータを用いてニューラルネットワークを学習し、その性能を検証した。
\end{jabstract}

\maketitle

\chapter{Introduction}
% For dinamic analysis, malware is executed in the sandbox and the behaviour is examined with the changes ocuuring in the sandbox. 
% Usually, there are two malware analysis approach, the dinamic analysis and the static analysis.
% In the static analysys, the behaviour of the malware is analysed by 

These days, the amount of malware has been increasing rapidly.
So the development of malware analysis tools, such as decompilers, has been becoming important.
A decompiler is a tool for analysing the behavior of malwares. 
This tool generates C-like pseudocode which represents the behaviour of the malware, and analyst can infer the behavior of the malware with the pseudocode.
This is much easier compare to analysing raw binary. 


But sometimes the decompiler generate  unstructured code, which calls for some effort to understand the behavior of the program. 
This is due to the flexibility of the high level language. If the decompiler chooses a human-friendly representation of the language as the result of decompiling, we can analyze malware with less effort. 

In this paper, we apply a statistical machine translation tequnique for a decompiler to generate human-friendly pseudocode. 
This idea was already mentioned by Katz et al. [1], in which they use end-to-end machine translation with Recurrent Neural Networks.

\section{Previous Studies}
\subsection{Decompilation techniques}

% Compiler and Optimization Level Estimation for Improving Anti-malware Technologies

% 

\subsection{Improvement of decompilation results}
In this paper, we don't try to recover the function name and variable name. 
But for hand-decompilation, sometimes the name can be estimated by the functionality of the variable.
For example, assume there is a variable X which seems to have a C struct type and has two fields P and Q.
And there are two functions F and G which operate with X. 
The function F recieves X and other value V, assigns V to the memory indicated by P, adds 8 to P and increments Q. 
The function G recieves X, subs 8 from P and decrements Q, then returns the value indicated by P.
In the situation, analyst can estimate X represents a stuck structure, and the function F is for push function, G is for pop function.

\cite{name_recover_from_decompile_result} is the paper for tuckling this task.



This kind of name estimation also appears in the context of deobfsucation.
Deobfsucation is a reverse process of obfsucation.
Obfsucation is a technique to modify the program source code so that the code becomes hard to understand for humans. 
Obfsucation is used for hiding the behaviour of a program whose source code have to be distributed.
For example, 


\subsection{Deep learning for programming task}


\chapter{Problem Settings and Methots}

\section{Decompiler}

% 573 words.

A compiler is a program which translates source code written in language X into other language Y. 
Usually, X is high-level language and Y is relatively low-level language. 
For example, clang compiler translates C language into x64 assembly language, javac compiler translates Java to Java virtual machine code.

A decompiler is a program which aims to reverse the process of a compiler, retrive a source code of the high-level language X from the source code of the low-level language Y. 
This reversing process is usually insufficient or impossible.
This is because many informaitions, such as variaible name or function name, are lost when compiling.
Additionaly, the program structure descripted in the high level language are lost too. 
For example, a simple {\sl for statement} can be represented by the combination of a {\sl while statement} and an {\sl if statement}, or the combination of {\sl if statement} and {\sl goto statement}. 
So a decompiler can't distinguish their representation difference, despite in the real situation {\sl for statement} are often used and {\sl goto statement} are rarely. 

Existing deterministic decompilers are made up with many steps similer to compilers. 
They find patterns in binary data, convert them to more high-level structure, and chain them so that they finally generate high-level psudecode.
Their output psudecodes are sometimes wrong or hard to understand.

% https://github.com/torvalds/linux/blob/70c25259537c073584eb906865307687275b527f/lib/rbtree.c#L528


\begin{figure}
	\begin{tabular}{cc}
		\begin{minipage}[t]{0.5\hsize}
			\lstinputlisting[]{rb_tree.c}
		\end{minipage}
		\begin{minipage}[t]{0.5\hsize}
			\lstinputlisting[]{hopper.c}
		\end{minipage}
		\\
		\begin{minipage}[c]{0.5\hsize}
			\lstinputlisting[]{snowman.c}
		\end{minipage}
		\begin{minipage}[c]{0.5\hsize}
			\lstinputlisting[]{handdecomp.c}
		\end{minipage}
	\end{tabular}
	\caption{ hogehugapiyo.}
	\label{fig:cw}
\end{figure}


Figure~\ref{fig:cw} shows the example decompilation result of the Hopper disassembler and the snowman decompiler and a hand decompilation for a C language code.
The output of Hopper decompiler seems not so bad, but some essential statements are missing and there are some verbose variables.
The output of snowman decompiler also seems broken. 
The snowman decompiler finds some variablels are pointer of a structure, but fails to detect the range of the function, and generates many unneccesary casts.
Idealy, the decompiler can find correct function ranges and structure of statements as a hand decompilation result, but the decompilers failed.

In this paper, we try to apply statistical approach, specifically SMT (statical machine translation) technique, for decompilation.
The decompilation problem is folmulated as follows. 
Given $sx$ as a source code of domain language X, compiler generates $sy$ as a code of target language Y with probability $p(sy|sx)$. 
We assume the prior distribution $p(sx)$ as the human-generated souce code. 
With that assumption, the provability of decompilation $p(sx|sy)$ is in proportion to $p(sy|sx)p(sx)$ according to the bayesian low. 
The $\argmax_{sx} p(sx|sy)$ is considered as the best human-intelligible decompilation result for the low-level code $sy$,  
so we try to estimate $ \argmax_{sx} p(sy|sx)p(sx)$ for decompilation result.
And if the compiler is deterministic, there wolud be a function $f$ which represents the compiler and $p(f(sx)|sx) = 1$,
then the objective becomes to $ \argmax_{sx,f(sx)=sy} p(sx)$.

The distribution $p(sx)$ is approximated by collecting source code from open source projects.
So if we had enough high-level source code data, we could choose the most popular high-level source code which generats given low-level code for the decompile result.
But high-level source code data can't be comprehensive.
So we have to model the structure of compilation process somehow.
We modeled the structure by the LSTM, which is commonly used in resent SMT techniques.

\section{LSTM}
To generate $ \argmax_{sx} p(sx|sy)$, we decomposit $ p(sx|sy) = p([sx_1, \dots, sx_n] |sy) $ to $ p(sx_1|sy,[]) \dots p(sx_n|sy,[sx_1,\dots,sx_{n-1}]) $,
where $ p(sx_k|sy,[sx_1,\dots,sx_{k-1}]) $ represents the probability that the token $ sx_k $ follows to the sequence $ [sx_1,\dots,sx_{k-1}] $ 
when the input sequence is $ sy $. We try to estimate the probability $ p(sx_k|sy,[sx_1,\dots,sx_{k-1}]) $ by LSTM.
LSTM is 


\chapter{Experiments}

\section{Data Collection}
% 219 words
First, we collected source codes by crawling \cite[GitHub]{github}, cloning about 900 repositories with C language topic, 
and recursevily searched each folders to enumerate C language source code.
From each source code, we generated pairs of C source code fragment and x64 assembly code corresponds to the C fragment.
Some sample flagments are shown in figure~\ref{fig:pairsofflagments}. 

% Here the reason for fragments?

\begin{figure}
	\begin{tabular}{cc}
	\end{tabular}
	\caption{Sample data flagments}
	\label{fig:pairsofflagments}
\end{figure}

The pairs were generated as follows. Let {\sl S0} be a original C source code.
\begin{enumerate}
\item Remove all preprocessor, such as {\sl \#include}, in {\sl S0} to generate preprosessed C code {\sl S1}. 
\item Tokenize {\sl S1} and rearrange them in {\sl S2} so that there is exactly one token for each lines. 
\item 
Compile {\sl S2} by gcc (GNU C compiler) with no optimization (-O0) and debug option to get assembly code {\sl S3}. 
With debug option, {\sl S3} contains informations about the source code position which each assembly operations are originated from.
\item Assemble {\sl S3} to object file {\sl S4} and disassemble {\sl S4} to get information eliminated assembly code {\sl S5}.
\item 
Parse {\sl S2} with clang to get AST (abstract syntax tree) of the source code. 
And for each subtree of AST, we can get C flagment source code which corresponds to the subtree, 
and we can get assembly code corresponds to the C flagment by employing {\sl S5} combine with {\sl S3}.
\end{enumerate}

The C flagment and assembly code pairs are preprocessed for the training and testing data.


\section{Data Preprocessing}

% 244 words
In order to apply LSTM, input and output for the LSTM must be the sequence of tokens. 
So we have to preprocess the assembly data and C flagments to the token sequences by somehow.

The assemblies were preprocessed in two ways.
First way is the raw binary integers. The assemblies are finally assembled to the sequence of the one byte integers, ranged from 0 to 255.
So the one byte integer sequences can be directly used for input sequences.
Second way is the tokenized sequence of assemby codes. 
The disassemble opecodes and operands in {\sl S5} are splited to symbols, and concatnated the sequences to one sequence, then we modified followings.
\begin{itemize}
\item For each end of the operations, the {\sl ENDLINE} symbol is added to indicate the end of operation.
\item 
The relative access of code address, such as {\sl call} or {\sl jump} operations, 
are relabeled like alpha-conversion for position independentness of the code and for reducing the vocabulary size.
\item The big integer values are substituted. 
In detail, the intager in the range from -255 to 256 are preverved as it is, and the other intengeres are substituted with the {\sl \_\_HEX\_\_} symbol.
This also reduces the vocabulary size.
\end{itemize}

The C source codes were preprocessed like the second way of the assembly preprosess.
C source code was tokenized to the sequence and preprocessed with following modifications.
\begin{itemize}
\item Function names and variable names were renamed like alpha-conversion.
\item Big integer values were substituted too. 
\item All string literals were replaced to the {\sl \_\_STRING\_\_} symbol.
\end{itemize}




\section{Model Configuration}
The training model was implemented with Chainer 6.0.0b1.
The 


\chapter{Results}
\chapter{Discussion}
\chapter{Conclusion}

\bibliographystyle{plainnat}
\bibliography{reference}

\end{document}
